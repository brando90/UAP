\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{bbm}

\title{EstimatorDB}

\author{Brando Miranda}

\begin{document}
\maketitle

\begin{abstract}
In the modern world of big data, some times it is too costly to go through all the data one may have in a database.
However, we are still interested in being able to query a database holding our data and get some understanding of the data that we have.
In this paper I propose three different methods to estimate the total amount of a quantity some group of you data may have.
First we will approximate the number of elements pertaining to one group and then, we will also estimate their mean value.
With these two approximated quantities, we can easily estimate the total amount one group contributes by multiplying both averages.
I will also argue the correctness of the algorithms that I propose.
In the end, we will evaluate each algorithm in practice by comparing how each of them do on real data.
\end{abstract}

\section{Introduction}

In recent years the amount of recorded data has increased exponentially.
With this size data, it has become impractical to even read all the data.
Thus, approximation tools have arisen to address this problem, so that users can still get understanding from their data in reasonable time.
In this paper we explore some of these different tools.\\
\indent To be able to explain these tools lets first go over some definitions used in this paper.
Let an attribute be the data of interest.
For example, an attribute can be the delay in hours for some airline.
Each attribute (delay) belongs to some group (i.e. the airline).
For example, for a series of flights, we might have a list of delays (attributes) for each airline (group).
Airlines and delays are just illustrative example and
I will mostly use the terms groups and attributes from here on.
In this paper the word counts will mean the number of elements that we have for some group.
To make clear what counts means consider the following simple example.
If we have two groups $g_1$ and $g_2$ and we have 3 attributes for $g_1$ and 7 attributes for $g_2$, then the counts for $g_1$ is 3 and the counts for $g_2$ is 7.\\
\indent In this paper I will suggest three ways of estimating counts for a group.
I will also prove that these methods are reasonable estimates.
The idea of what the algorithms are is easy and intuitive.
The "easy" (simpler) algorithm just selects multiple subsections of the data and from those subsections estimates the fraction of elements that belong to some group $g_i$. \\
\indent The less obvious algorithms for estimating counts works a little different.
The less obvious algorithms depends on modeling the occurrences of a group as a Poisson process.
It models the idea that, as we loop through an array of data, the longer time that we take to see the first occurrence of a certain attribute belonging to some group, the less frequent that group probably is (the less that group occurs in the data).
The idea of the first algorithm is to start at different indexes in the data, and record the amount of time it takes to see $n$ fixed occurrences of a specific group.
With this information, then one can come up with a metric that to estimate the mean of the Poisson distribution.
Two really nice properties of this estimate that I will show is that it is unbiased and consistent.\\
\indent The second algorithm for estimating counts also depends on the same intuition that the longer the time to see the occurrence of a group, the less frequent the group probably is in the data set.
This second estimator is the regular MLE estimate for Poisson estimates.
We allow the loop to run for some fixed time and then count the number of times we see the occurrence of some group g. \\
\indent In the end, the goal will be to run these algorithms with real data and see which ones perform better in practice and thus conclude if a Possoin distribution is a better model or if a Binomial distribution is a better model

\section{Statistical Estimates}

Some of these estimates are obvious that they should work in theory, but I will verify it with two very simple proofs and one less simple one.
I will start with the simple ones.
Throughout this section let:

$\hat{c}$ be the counts we are trying to estimate and $N$ be the total amount of data (groups and their attributes that we have). In fact, one can thing of the data as being a really large array of tuples of $(g_i, a_{i})$.

\subsection{Estimate 1}

\subsubsection{Definitons}

Let $g$ be a fixed group that we want to know the counts for.
Let $G_i$ be the indicator variables such that, its 1 if the element i on the data is of group g, and 0 o.w.


Thus, if we wish to estimate the total number of elements in the data set that are of group g then we could compute it multiplying the probability of finding element $g_i$ times the total number of elements in the data. 
If we have a subsection of the data $Data[i:j]$ then we can count the number of times group g occurred on the that interval and then use that as an estimate for probability of finding group $g$.
Let k be the number of elements in the interval i to j.
To formalize let $p_g$ be the true probability of finding group g.
To formalize this let $\hat{p}_g$ be the estimated probability of finding group g:

$$ \hat{p}_g = \frac{1}{k} \sum^{k}_{i=1} G_i $$

Then we have the estimate to be:

$$ \hat{c} = \hat{p}_g \cdot N $$

Lets check that this estimate is unbiased and consistent:

$$E[ \hat{c} ] = N \cdot E[\frac{1}{k} \sum^{k}_{i=1} G_i ]$$
$$N \cdot \frac{1}{k} \sum^{k}_{i=1} E[G_i ] = N \cdot p_g$$

This last line shows that if we assume the data is Binomial, then this estimate is unbiased and consistent.
Obviously, one can just get multiple estimates of $\hat{c}$ and average them and the estimate is still valid.

\subsection{Estimate 2}

In this section I will describe one of the estimators under the assumption that finding a g is a random process.
The estimate in this section is the usual MLE estimate for Poissons.
Let $\lambda$ be the usual Poisson mean (the number of times we see group g in a unit time, say 1 second).
Fix the amount of time $t$ that you are going to run the loop to find the element g.
Let $N_t$ be the random variable counting the number of times we see g in a time interval [0,t].

Then we can estimate the average amount of times we expect to see g in a time interval t in the following way

$$ \lambda = \frac{N_t}{t}$$

Then one can show that this is a reasonable estimate:

$$E[\hat{\lambda}] =\frac{1}{t}E[N_t]=\frac{1}{t}\lambda t=\lambda$$

Obviously, one can just get multiple estimates of $\hat{\lambda}$ and average them and the estimate is still valid.

Let $T$ be the total time it takes to loop through all the data.

Then if we had this value, then we could easily estimate the total number of elements that we would expect to see for group g in the whole data. Thus the estimate would be:

$$\hat{c} = \lambda \cdot T$$

The problem comes when we have to estimate T, because the whole point is to not iterate the whole data.
However, for testing purposes we could just read the whole data a couple of times to see get an average estimate for T.
However, this solution is rather unsatisfactory when we actually want to use this in practice, because the whole point is to process and read as little data as possible (and get the most information that we can). 
Could we have done something else?
We could try to approximate T by a different method.
Let $n_t$ be the number of elements we traverse in a time period t (we could take advantage and count $n_t$ as we are counting the occurrences of g in this time interval, i.e. $N_t$).
Then the average time to traverse 1 element would be:

$$\frac{n_t}{t}$$

But we know the total data that we have, so we can estimate $T$ in the following way:

$$\hat{T} = \frac{t}{n_t} \cdot n$$

Note that we can get a better estimate for both, $\hat{\lambda}$ and $\hat{T}$ by running it more time and calculating an average.


Now we can estimate the counts with the following formula:

$$\hat{c} = \lambda \cdot \hat{T} $$
$$\hat{c}  = \lambda \cdot \frac{t}{n_t} \cdot n$$

\subsection{Estimate 3}

In this section I will describe one of the estimators under the assumption that finding a g is a random process.
This estimate is inspired from the MLE, but without careful treatment of it, can lead to a estimate that is biased but consistent.
To show this estimate, I iterate on how I got to it.
The reason this estimate came up was because I was thinking about the following question, what if I wanted the user to not have to specify a time in advance of how long the algorithm has to run but instead how many occurrences of g they want to see?
With this in mind, you can instead search for the first occurrence of g from different offsets on you data and record how long it took to find each one.
With this in mind, let $t_i$ be the time it too to find the first g.
Note that if the occurrences is model as a Poisson, then $t_i \sim Exp(\lambda)$.
Let $T_i$ be the actual time that this occurrence was recorded (so say $T_i$ could be 4pm etc).
Let $T_0 = 0$ be the initial time and $T_k$ the last time a g was seen.
Then, $T_i \sim Gamma(n, \lambda)$.
Now the naive algorithm to run is, choose k different offsets and see how long each one takes to find the occurrence of g and record that as $t_i$.
Thus, we could just record the total time to see all of these $k$ g's as: 
$$ T_k = \sum^{k}_{i=1} t_i$$

Thus our estimate of $\lambda$ be calculated as:

$$\hat{\lambda} = \frac{k}{\sum^{k}_{i=1} t_i}$$

But is this a valid estimate?

Well lets see what the expectation of this estimate is.
Note the Since $T_k$ is a sum of $n$ independent exponential random variables, then $T_k\sim\mathrm{Gamma}(k,\lambda)$

$$E[\hat{\lambda}] = E[ \frac{k}{ \sum^{k}_{i=1} t_i } ]$$

$$E[\hat{\lambda}] = E[ \frac{k}{ T_k } ]$$

Since $T_k$ is a sum of $k$ independent exponential random variables, then $T_k\sim\mathrm{Gamma}(k,\lambda)$. That is, the density of $T_k$ (with respect to the Lebesgue) measure is given by:

$$
f_n(x)=\frac{x^{n-1}}{\Gamma(n)}\lambda^ne^{-\lambda x} 1_{(0,+\infty)}(x).
$$

We can proceed to calculate the expectation:

$$
 E\left[\hat\lambda\right]=k\int_0^\infty\frac{x^{k-2}}{\Gamma(k)}\lambda^ke^{-\lambda x}\, dx.
$$

The integral diverges for $k=1$ (because of the $x^{k-2}$).
Therefore for $k\ge2$  you can compute the integral as

$$
 E\left[\hat\lambda \right]=k\frac{\lambda\Gamma(k-1)}{\Gamma(k)}\underbrace{\int_0^\infty\frac{x^{k-2}}{\Gamma(k-1)}\lambda^{k-1}e^{-\lambda x}\, dx}_{=1}
$$

Recall $\Gamma{(k)} = (k-1)!$ when k is an integer

$$
E[\hat{\lambda}] =  k\frac{\lambda (k-2)! }{(k-1)!}= \frac k{k-1}\lambda.
$$

Which isn't equal to $\lambda$. 
But this estimate is not completely unreasonable.
It has the nice property that it converges to the correct $\lambda$ as k goes to infinity.
But its a little unfortunate to have to need that many samples.
Instead we can do better by re-defining our estimator to be a different quantity that is un-biased and consistent.
The quantity we want is:

$$
\hat\lambda =\frac{k-1}{\sum_{i=1}^k t_i},
$$


Now we have much better estimate for $\lambda$.
Now we can calculate the counts easily in a similar manner as in the previous section:

$$\hat{c} = \hat{\lambda} \cdot \hat{T} $$

For this we need the estimate of $\hat{T}$.
This can be estimated by estimating the average time to read through all the k data while we were searching for g. :

$$ \frac{ \sum^{k}_{i=1} t_i }{ \sum^{k}_{i=1} n_i}$$

The quantity above intuitively represents the total amount of time to traverse 1 element.
Thus, we can use the above quantity to estimate the total amount of time traverse the whole data if we do:

$$\hat{c}  = \lambda \cdot \frac{\sum^{k}_{i=1} t_i}{\sum^{k}_{i=1} n_i} \cdot n $$

\subsection{Estimating total amount of an attribute}

Recall that a group has a list of attributes.
Now that we know the counts for an attribute, its easy to calculate the total amount for an attribute since, we can get the mean estimate for an attribute using the ifocus algorithm suggested by Albert Kim.
Let the mean of an attribute for a group g be denoted by $\hat{\mu}_g$. 
Let the total for a group a be denoted by $\tau_g$.
Let the estimated count for a group be denoted $\hat{c}_g$
Once we have that we calculate the total by multiplying the average by the number of elements that we expect the data to have:

$$\hat{ \tau}_g = \hat{c} \cdot \hat{\mu_g} $$


%%----------------

\section{Experiments}

Let $\lambda_1$, $\lambda_2$ and $\lambda_3$ stand for the estimator 1, 2 and 3 from the previous section.
The following data are evaluations on the algorithm after being run of synthetic data.

\begin{tabular}{l*{6}{c}r}
  & MPE counts & MPE total  \\
\hline
$\lambda_1$ & 6 & 4  \\
$\lambda_2$  & 6 & 3   \\
$\lambda_3$  & 6 & 2    \\
\end{tabular}

\begin{tabular}{l*{6}{c}r}
A  & P & W & D  Pts \\
\hline
  & 6 & 4 & 0  \\
B  & 6 & 3 & 0   \\
B  & 6 & 2 & 1   \\
B  & 6 & 2 & 1  \\
\end{tabular}



\section{Conclusion}



\end{document}