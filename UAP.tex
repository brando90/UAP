\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{bbm}

\title{EstimatorDB}

\author{Brando Miranda}

\begin{document}
\maketitle

\begin{abstract}
Increasingly, databases are storing more and more data, making it costly to go through all the data one may have in a database.
However, users are still interested in being able to query a database holding their data to get some understanding of the data that they have. %
In this paper I propose three different sampling-based methods to estimate the total mean value of one particular attribute in a particular group of records in a data set. %
First we approximate the number of elements pertaining to one group and then, estimate their mean value.
With these two approximated quantities, we can easily estimate the total amount one group contributes by multiplying both averages.
I will also argue the correctness of the algorithms that I propose.
We evaluate each algorithm in practice by comparing them on real data.
\end{abstract}

\section{Introduction}

In recent years the amount of recorded data has increased exponentially.
Making it increasingly impractical to even read all the data.
Thus, approximation tools have arisen to address this problem, so that users can still get understanding from their data in reasonable time.
In this paper we explore some of these different tools.\\
\indent To understand these tools we begin with a few definitions.
Let an attribute be the data of interest.
For example, an attribute can be the delay in hours for some airline.
Each attribute (delay) belongs to some group (i.e. the airline).
For example, for a series of flights, we might have a list of delays (attributes) for each airline (group).
In this paper the word counts will mean the number of elements that we have for some group.
To make clear what counts means consider the following simple example.
If we have two groups $g_1$ and $g_2$ and we have 3 records in $g_1$ and 7 records in $g_2$, then the counts for $g_1$ is 3 and the counts for $g_2$ is 7.\\
\indent In this paper I describe three ways of estimating the counts for a group.
I will also experimentally demonstrate that these methods are reasonable estimates.
The simpler algorithm just selects multiple subsections of the data and from those subsections estimates the fraction of elements that belong to some group $g_i$. Later I will show that this process leads to modeling the occurrences of groups as a Binomial distribution\\
\indent The less obvious algorithms for estimating counts works a little differently.
Specifically, they model the occurrences of a group as a Poisson process using the idea that as we loop through an array of data, the longer time that we take to see the first occurrence of a certain attribute belonging to some group, the less frequent that group probably is (the less that group occurs in the data).
The idea of the first algorithm is to start at different indexes in the data, and record the amount of time it takes to see $n$ occurrences of a specific group.
With this information, then one can come up with an estimate the mean of means of the Poisson distribution.
This approach has the desirable property that it estimates are both {\it unbiased} and {\it consistent}.\\
\indent The second algorithm for estimating counts also depends on the intuition that the longer the time to see the occurrence of a group, the less frequent the group probably is in the data set.
This second estimator is the regular Maximum Likelihood Estimate (MLE) estimate for Poisson processes.
We allow the loop to run for some fixed time and then count the number of times we see the occurrence of some group g. \\
\indent In the end, the goal will be to run these algorithms with real data and see which ones perform better in practice and thus conclude if a Poisson distribution is a better model or if the Binomial distribution is a better model.

\section{Statistical Estimates}

In this section, I describe the theory of the estimators I use, along with proofs of their correctness.
I will start with the simple ones.
Throughout this section let:

$\hat{c}$ be the counts we are trying to estimate and $N$ be the total amount of data (groups and their attributes that we have). In fact, one can think of the data as being a large array of tuples of $(g_i, a_{i})$.

\subsection{Estimate 1}

Let $g$ be a fixed group that we want to know the counts for.
Let $G_i$ be an indicator variables indicating the presence of a group.


Thus, we can estimate the total number of elements in the data set that belong to group g by multiplying the probability of finding element $g_i$ times the total number of elements in the data. 
If we have a subsection of the data $Data[i:j]$ then we can count the number of times group g occurs in the interval and then use that as an estimate for probability of finding group $g$.
Let k be the number of elements in the interval i to j.
To formalize this let $\hat{p}_g$ be the estimated probability of finding group g:

$$ \hat{p}_g = \frac{1}{k} \sum^{k}_{i=1} G_i $$

Then we have the estimate to be:

$$ \hat{c} = \hat{p}_g \cdot N $$

Lets check that this estimate is unbiased and consistent:

$$E[ \hat{c} ] = N \cdot E[\frac{1}{k} \sum^{k}_{i=1} G_i ]$$
$$N \cdot \frac{1}{k} \sum^{k}_{i=1} E[G_i ] = N \cdot p_g$$

This last line shows that if we assume the data is Binomial, then this estimate is {\it consistent} \footnote{In the statistics community, if an estimator is consistent it means that as the number of data points increases indefinitely, the estimate converges in probability to the desired quantity.} and {\it un-biased} \footnote{In the statistics community, bias means the difference of between an estimate and its expected value. If the difference is zero, then the estimate is un-biased.}.
Obviously, one can just get multiple estimates of $\hat{c}$ so that average them and the estimate is still valid.

\subsection{Estimate 2}

In this section I will describe the 2nd estimator under the assumption that finding a g is a random process.
The estimate in this section is the usual Maximum Likelihood Estimate estimate for Poissons.
Let $\lambda$ be the usual Poisson mean (the number of times we see group g in a unit time, say 1 second).
Fix the amount of time $t$ that the process iterates over the data looking for elements in g.
Let $N_t$ be the random variable counting the number of times we see g in a time interval [0,t].

Then we can estimate the average number of times we expect to see g in a time interval t in the following way

$$ \lambda = \frac{N_t}{t}$$

Then one can show that this is a reasonable estimate:

$$E[\hat{\lambda}] =\frac{1}{t}E[N_t]=\frac{1}{t}\lambda t=\lambda$$

Obviously, one can also get multiple estimates of $\hat{\lambda}$ and average them and the estimate will still be valid.

Let $T$ be the total time it takes to loop through all the data 
(An estimate for $T$ will be given in section 2.3).

Then if we had this value, then we could easily estimate the total number of elements that we would expect to see for group g in the whole data. As:

$$\hat{c} = \lambda \cdot T$$


\subsection{Estimating time to traverse the whole data}

The problem comes when we have to estimate T, because the whole point is to not iterate through the whole whole data.
One simple approach would be to scan all of the data a few times to get an average estimate for T.
However, this solution is rather unsatisfactory when we actually want to use the previous estimator in practice, because the whole point is to process and read as little data as possible (and get the most information that we can). 
Could we have done something else?
In fact $T$ using a different method.
Let $n_t$ be the number of elements we traverse in a time period t (we can and count $n_t$ as we are counting the occurrences of g in this time interval, i.e. $N_t$).
Then the average time to traverse 1 element would be:

$$\frac{t}{n_t}$$

(note the quantity above estimates the time interval that passes by when we traverse 1 element) We could make this estimate better by repeating it many times:

$$\frac{\sum^{n}_{i=1} t_i}{\sum^{n}_{i=1}n_{t, i}}$$

 Since we know the total number of elements that we have, we can estimate $T$ in the following way:

$$\hat{T} = \frac{\sum^{n}_{i=1} t_i}{\sum^{n}_{i=1}n_{t, i}} \cdot n$$

Now we can estimate the counts with the following formula:

$$\hat{c} = \lambda \cdot \hat{T} $$
$$\hat{c}  = \lambda \cdot \frac{\sum^{n}_{i=1} t_i}{\sum^{n}_{i=1}n_{t, i}} \cdot n$$

\subsection{Estimate 3}

In this section I describe the 3rd estimator, which is also based on the assumption that finding a group g is a random process.
This estimate is inspired from the MLE, but without careful treatment of it, can lead to an suboptimal estimate (i.e. one that is biased but consistent).
I will develop this estimate iteratively.
Suppose wanted the user to not have to specify a time in advance of how long the algorithm has to run but instead how many occurrences of g they want to see?
With this in mind, we can instead search for the first occurrence of g from different offsets in the data and record how long it takes to find each one.
With this in mind, let $t_i$ be the time it takes to find the first g.
Note that if the occurrences are modeled as a Poisson, then $t_i \sim Exp(\lambda)$.
Let $T_i$ be the actual time when this occurrence was recorded.
Let $T_0 = 0$ be the initial time and $T_k$ the last time a g was seen.
Then, $T_i \sim Gamma(n, \lambda)$.
Now a naive algorithm could, choose k different offsets and see how long each one takes to find an occurrence of g and record that as $t_i$.
Thus, we could just record the total time to see all of these $k$ g's as: 
$$ T_k = \sum^{k}_{i=1} t_i$$

Thus our estimate of $\lambda$ be calculated as:

$$\hat{\lambda} = \frac{k}{\sum^{k}_{i=1} t_i}$$

But is this a valid estimate?

Let's see what the expectation of this estimate is.
Note the Since $T_k$ is a sum of $n$ independent exponential random variables, then $T_k\sim\mathrm{Gamma}(k,\lambda)$

$$E[\hat{\lambda}] = E[ \frac{k}{ \sum^{k}_{i=1} t_i } ]$$

$$E[\hat{\lambda}] = E[ \frac{k}{ T_k } ]$$

Since $T_k$ is a sum of $k$ independent exponential random variables, then $T_k\sim\mathrm{Gamma}(k,\lambda)$. That is, the density of $T_k$ is given by:

$$
f_n(x)=\frac{x^{n-1}}{\Gamma(n)}\lambda^ne^{-\lambda x}
$$

We can proceed to calculate the expectation:

$$
 E\left[\hat\lambda\right]=k\int_0^\infty\frac{x^{k-2}}{\Gamma(k)}\lambda^ke^{-\lambda x}\, dx.
$$

The integral diverges for $k=1$ (because of the $x^{k-2}$).
However, for $k\ge2$  we can compute the integral as

$$
 E\left[\hat\lambda \right]=k\frac{\lambda\Gamma(k-1)}{\Gamma(k)}\underbrace{\int_0^\infty\frac{x^{k-2}}{\Gamma(k-1)}\lambda^{k-1}e^{-\lambda x}\, dx}_{=1}
$$

Recall $\Gamma{(k)} = (k-1)!$ when k is an integer

$$
E[\hat{\lambda}] =  k\frac{\lambda (k-2)! }{(k-1)!}= \frac k{k-1}\lambda.
$$

Which isn't equal to $\lambda$ (meaning there is some bias in the estimate). 
But this estimate is not completely unreasonable.
It has the nice property that it converges to the correct $\lambda$ as k goes to infinity.
But it's a little unfortunate that it requires an infinite number of samples.
Instead we can do better by re-defining our estimator to be a different quantity that is {\it un-biased} and { \it consistent}.
The quantity we want is:

$$
\hat\lambda =\frac{k-1}{\sum_{i=1}^k t_i},
$$


Now we have much better estimate for $\lambda$.
We can calculate the counts easily in a similar manner as in the previous section:

$$\hat{c} = \hat{\lambda} \cdot \hat{T} $$

\subsection{Estimating total amount of an attribute}

Recall that a group has a list of attributes.
Now that we know the counts for an attribute, its easy to calculate the total amount for an attribute since, we can get the mean estimate for an attribute using the ifocus algorithm suggested by Albert Kim.
Let the mean of an attribute for a group g be denoted by $\hat{\mu}_g$. 
Let the total for a group a be denoted by $\tau_g$.
Let the estimated count for a group be denoted $\hat{c}_g$
Once we have that we calculate the total by multiplying the average by the number of elements that we expect the data to have:

$$\hat{ \tau}_g = \hat{c} \cdot \hat{\mu_g} $$

%%----------------

\section{Experiments}
In this section I briefly describe an experiment I conducted to compare the quality of these estimators.
Let $\lambda_1$, $\lambda_2$ and $\lambda_3$ stand for the estimator 1, 2 and 3 from the previous section.
$\hat{T}$ was estimated as discussed on section 2.3.

I will be evaluating these algorithms using the error (Mean Absolute Percentage Error) defined as:

$$\frac{1}{n}\sum^{n}_{i=1}\frac{|e_i - t_i|}{t_i}$$

where $e_i$ is the estimated value for test i and $t_i$ is the true value for test i.
The reason that I choose relative metric rather than absolute metric is because each different group has a different true number of counts and a true mean.
Thus, adding the error on different means makes little sense because they are on different scales.
Therefore, to remove that problem I decided to use the average absolute relative error.

The following data are evaluations on the algorithm after being run on synthetic data. 
The synthetic data was generated using the Bernoulli distribution and a Mixture of Gaussian.
For the Bernoulli generation of data, each group would have its own Bernoulli distribution and a data sample would be generated from each group. 
So an data sample for a group g would be sampled from $d_{i,g} \sim Bernouilli(p_g)$.
For the mixture of Gaussian, each group would have a probability of occurring $p_g$ and their own mean and standard deviation. 
Thus leading the the well known Mixture of Gaussian:

$$P(d_{i,g}) = \sum^{|G|}_{g=1} p_g N(d_{i,g} ; \mu_g, \sigma_g)$$

Note, G is the set of possible groups $G = \{ g_1, ..., g_{|G|} \}$.
\\

Table 1: Result on data sampled from a Bernoulli distribution:

\begin{tabular}{l*{6}{c}r}
  & MAPE counts & MAPE total  \\
\hline
$\lambda_1$ & 7.92\% &  7.96\% \\
$\lambda_2$  & 20.1\% & 20.2\%   \\
$\lambda_3$  & 30.2\% & 30.1\%    \\
\end{tabular} \\ \\



Table 2: Results on data sampled from a Mixture of Gaussian:

\begin{tabular}{l*{6}{c}r}
  & MAPE counts & MAPE total  \\
\hline
$\lambda_1$ & 7.19 \% &  7.21\% \\
$\lambda_2$  & 32.7\% & 32.6\%   \\
$\lambda_3$  & 22.3\% & 22.5\%    \\
\end{tabular}

The MAPE counts is defined as the relative percentage difference of the real count from the true counts:

$$\frac{1}{n}\sum^{n}_{i=1}\frac{|\hat{c_i} - c_i|}{c_i}$$

Where $\hat{c_i}$ is the estimated value of the counts while $c_i$ is the true number of counts.

The total is defined as the total $\tau$ amount for a group as defined in section 2.3.  

$$\frac{1}{n}\sum^{n}_{i=1}\frac{|\hat{\tau_i} - \tau_i|}{\tau_i}$$

The way that these results were obtained was that for 9 different groups there were 100000 different attributes, then we calculated the estimated counts and total amount of that attribute.
I repeated that 100 times and then I estimated the MAPE.

\section{Evalutation}

From the results, we can see that the estimator that derived from the binomial classifier seems to be the best from the three that I suggested.
It is not very surprising that it was the best when the data was generated as a Bernoulli process (since the Bernoulli is a special case of the Binomial).
One of the reasons we believe its better is because it takes a simpler approach at the estimation and avoid one one extra parameter of estimation.
The estimators that were derived from the Poisson processes also need an estimate for the total amount of time to traverse the whole data (because otherwise we only have the average amount of occurrences of a group per unit time but we don't really know how many elements in the data belong to some group).
The problem is that this introduced an extra uncertainty when it came to estimate counts. 
This extra error was not introduced in the $\lambda_1$, which might be one of the reasons it performed better.
One reason the binomial might be better is because, the binomial address the problem more directly.
The binomial is usually models the number of success in n trials.
The trials in this case is the number of times we see a specific group g in a iteration (trial) i.
The binomial distribution has a theoretical upper bound of n (i.e. we can only see at most n successes in n trials).
Which are properties that the problem we are trying to solve have.
We want to get an estimate for the number of occurrences for a group g (i.e. the success of finding that group) and the binomial seems to model it more directly.
While the Poisson does not have such an upper bound and we have to estimate quantities that are not directly relevant to the problem.
For example, in the Poisson we have a probably of seeing a specific group in a fraction of a second, while that is impossible to have in a Binomial, because the trials are discrete (while they are not discrete in the Poisson).
This ultimately complicates the model more than we need because we are modeling a problem that we know is discrete into a continuous frame work, which will ultimately bring errors in our calculations. \\
From the results of experiment 1 even though $\lambda_2$ seemed to be a better classifier than $\lambda_3$,  that was not true for the second experiment. 
Concluding that $\lambda_2$ is always better might be overfitting.
Therefore, since they perform in similar ways on both experiments but on the opposite direction (i.e. one does better when the other does better), it is conservative to conclude that the methods are basically similar and that you would get with $50\%$ chance the one that performs a little worse. 
Or on average, they both have about $25\%$ error.
This conclusion is reasonable considering that the mathematical derivations conclude that the classification methods basically have the same expectation, so they should produce similar results.


\section{Conclusions}

In conclusion, from the empirical results, it is clear that the estimator $\lambda_1$ is by far the best, beating the other estimates by more than $10\%$.
This probably means that trying to model the occurrences of a group first through a time event driven process is not the best way to model approximate the occurrence the occurrence of some event.
As I mentioned before, one of the problems that the Poisson process had was that we needed to estimate another variable, before being able to estimate the actual quantity that was of interest in hand.
It was inefficient to have to model some other event rather than addressing the problem directly.
This is one of the reasons I believe the binomial was a better method, since it did not circumvent the problem and addressed it more directly.
Furthermore, if we had any uncertainty on the quantity two quantities $\hat{c}$ and $\hat{T}$, then the uncertainty of each one would amplify our uncertainty on the final estimate of interest.
Therefore, the less quantities that we need to estimate to get our desired quantity, the better, because the less uncertainty we accumulate on the final results. 

\section{Future Work}

One key topic for future exploration, is to evaluate different method to estimate $\hat{T}$.
Is it just that the method that I currently have is not the best?
Also, having a more formal proof for the adequacy of the estimate for $\hat{T}$ would have also been very satisfying.
Also, further experiments on more different types of data would have made it more interesting, to see if estimate had a different advantage over the other, depending how the data was generated.
Specifically I would like to try a Normal, Multi-normals and any other process.
If the binomial was indeed the best for any situation, is there a proof out there that shows it out performs the Poisson for any data set?
Or maybe the structure we believe the data has can give us extra power to decide when each estimator is the appropriate one.




\end{document}